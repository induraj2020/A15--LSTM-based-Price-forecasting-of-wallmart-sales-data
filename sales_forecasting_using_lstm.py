# -*- coding: utf-8 -*-
"""Sales forecasting using LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oY9idt_GsmvgDgZhbBBsJJLe4iMIk1ey
"""

#!pip install tf-nightly

import tensorflow as tf
print(tf.__version__)

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"

!pip install tensorflow==1.15

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

from tensorflow import keras # version 2.2.4-tf
import numpy as np
from keras.preprocessing.sequence import TimeseriesGenerator
print(keras.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense, CuDNNLSTM, Dropout
from keras.layers.advanced_activations import LeakyReLU
from keras.callbacks import EarlyStopping

from google.colab import drive
drive.mount('drive')

!ls

# from google.colab import files
# files.upload()

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

print(spark)
filelocation1 = "/content/drive/My Drive/data/train.csv"
filelocation3 = "/content/drive/My Drive/data/stores.csv"
filelocation4 = "/content/drive/My Drive/data/features.csv"

df1 = (spark.read.option('header','true').option("inferSchema", "true").csv(filelocation1))
df3 = (spark.read.option('header','true').option('delimiter',",").option("inferSchema", "true").csv(filelocation3))
df4 = (spark.read.option('header','true').option('delimiter',",").option("inferSchema", "true").csv(filelocation4))

df = df1.join(df3,on=["Store"],how='inner')

df_final= df.join(df4,on=["Store","Date","IsHoliday"],how="inner")

df_pandas=df1.toPandas()

df_pandas['Date'] = pd.to_datetime(df_pandas['Date'])
print("The dataset has {} number of days data".format(((df_pandas['Date'].max()-df_pandas['Date'].min())/np.timedelta64(1,'D'))))
print("which is equal to {} months of data".format((df_pandas['Date'].max()-df_pandas['Date'].min())/np.timedelta64(1,'M')))
print("equal to {} years of data".format((df_pandas['Date'].max()-df_pandas['Date'].min())/np.timedelta64(1,'Y')))
print("2010 has {} rows of data".format(len(df_pandas[df_pandas['Date'].dt.year==2010])))
print("2011 has {} rows of data".format(len(df_pandas[df_pandas['Date'].dt.year==2011])))
print("2012 has {} rows of data".format(len(df_pandas[df_pandas['Date'].dt.year==2012])))

df_pandas.groupby('IsHoliday').IsHoliday.count().plot(kind='pie',title='Proportion of Holidays vs Non holiday')

from pylab import rcParams
rcParams['figure.figsize'] = 8, 5
#df_pandas.groupby(['Store','IsHoliday','Date']).Weekly_Sales.mean().plot()
df_pandas.groupby(['IsHoliday']).Weekly_Sales.mean().plot()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig_dims = (15, 5)
fig, ax = plt.subplots(figsize= fig_dims)
df_pandas.groupby('Store').Weekly_Sales.mean().sort_values(ascending=False).plot(kind='bar',title='Average weekly sales by Store - TOP PROFITABLE STORES' )

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig_dims = (35, 5)
fig, ax = plt.subplots(figsize= fig_dims)
df_pandas.groupby('Date').Weekly_Sales.mean().plot(kind='line',title='Sales over the period of 3 years')

df_pandas_1 = df_pandas.set_index(['Date'])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig_dims = (10, 5)
fig, ax = plt.subplots(figsize= fig_dims)
df_pandas_1.groupby('IsHoliday').Weekly_Sales.mean().plot(kind='pie',title='Holiday Sales vs Non Holiday Sales')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig_dims = (35, 8)
fig, ax = plt.subplots(figsize= fig_dims)
df_pandas_1.groupby(['Store','IsHoliday']).Weekly_Sales.mean().plot(kind='bar', legend=True,title='Weekly sales by compaby and holiday efffect')

# import warnings
from datetime import datetime
# import itertools
import statsmodels.api as sm

df_pandas_2= df_pandas.copy()
df_pandas_2 = df_pandas_2.groupby('Date').Weekly_Sales.mean().reset_index()
df_pandas_2['Date'] = pd.to_datetime(df_pandas_2['Date'])
df_pandas_2 = df_pandas_2.set_index('Date')
df_pandas_2.index = pd.to_datetime(df_pandas_2.index)

#plt.plot(df_pandas_2['Date'],df_pandas_2['Weekly_Sales'],kind='line')
decomposition = sm.tsa.seasonal_decompose(df_pandas_2['Weekly_Sales'], model='additive',freq=50)
fig = decomposition.plot()
fig.set_size_inches(18.5, 10.5)
plt.show()



from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer

df_final = df_final.withColumn("CPI",df_final.CPI.cast("double"))
df_final = df_final.withColumn("Unemployment",df_final.Unemployment.cast("double"))
df_final = df_final.withColumn("MarkDown1",df_final.Unemployment.cast("double"))
df_final = df_final.withColumn("MarkDown2",df_final.Unemployment.cast("double"))
df_final = df_final.withColumn("MarkDown3",df_final.Unemployment.cast("double"))
df_final = df_final.withColumn("MarkDown4",df_final.Unemployment.cast("double"))
df_final = df_final.withColumn("MarkDown5",df_final.Unemployment.cast("double"))
df_final = df_final.withColumn("IsHoliday",F.when(df_final.IsHoliday=='false',F.lit(0)).otherwise(F.lit(1)))
indexer = StringIndexer(inputCol="Type", outputCol="Type_ind").fit(df_final)
df_final = indexer.transform(df_final)
cols_to_drop=["Type","Date"]
df_final1 = df_final.drop(*cols_to_drop)
df_final1.show()

df_final1= df_final1.select(['Weekly_sales','Store','IsHoliday','Dept','Size','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment','Type_ind'])

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np 

values = df_final1.collect() 

scaler = StandardScaler()
scaled = scaler.fit_transform(values)

x= scaled[:][:-48]
y= scaled[:,0][48:]

from sklearn.model_selection import train_test_split

# split into train and test sets
trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.20, random_state=42, shuffle = False)

# Create overlapping windows of lagged values for training and testing datasets
timesteps = 12
train_generator = TimeseriesGenerator(trainX, trainY, length=timesteps, sampling_rate=1, batch_size=timesteps)
test_generator = TimeseriesGenerator(testX, testY, length=timesteps, sampling_rate=1, batch_size=timesteps)

import tensorflow 
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import LeakyReLU
from keras.callbacks import EarlyStopping
from keras.layers import LSTM

from tensorflow.keras import backend
import numpy as np
units = 128
num_epoch = 5
learning_rate = 0.001

model = Sequential()
model.add(LSTM(units, input_shape=(12, 15)))
model.add(LeakyReLU(alpha=0.5)) 
model.add(Dropout(0.1))
model.add(Dense(1))

adam = Adam(lr=learning_rate)

model.compile(loss='mse', optimizer=adam, metrics=['mae'])

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model.png', show_shapes=True)
from IPython.display import Image
Image(filename='model.png')

model.fit_generator(train_generator, epochs=10).history

# Make predictions

import numpy as np

yhat_train_temp = model.predict_generator(train_generator)
yhat_test_temp = model.predict_generator(test_generator)

n_lead = 1
yhat_train = yhat_train_temp[:, n_lead-1]
yhat_test = yhat_test_temp[:, n_lead-1]

# training results
yhat_train_plot = np.empty(shape=[y.shape[0],])
yhat_train_plot[:] = np.nan
yhat_train.shape = yhat_train.shape[0]
yhat_train_plot.shape = yhat_train_plot.shape[0]
yhat_train_plot[timesteps:len(yhat_train)+timesteps] = yhat_train
  
#test results
yhat_test_plot = np.empty(shape=[y.shape[0],])
yhat_test_plot[:] = np.nan
yhat_test.shape = yhat_test.shape[0]
yhat_test_plot.shape = yhat_test_plot.shape[0]
yhat_test_plot[len(yhat_train)+(timesteps*2):len(y)] = yhat_test

from matplotlib import pyplot

fig = pyplot.figure()
pyplot.style.use('seaborn')
palette = pyplot.get_cmap('Set1')
pyplot.plot(y, marker='', color=palette(4), linewidth=1, alpha=0.9, label='actual')
pyplot.plot(yhat_train_plot, marker='', color=palette(2), linewidth=1, alpha=0.9, label='training predictions')
pyplot.plot(yhat_test_plot, marker='', color=palette(3), linewidth=1, alpha=0.9, label='testing predictions')

pyplot.title('Forecast', loc='center', fontsize=20, fontweight=5, color='orange')
pyplot.ylabel('Sales')
pyplot.legend()
fig.set_size_inches(w=15,h=5)
pyplot.close()

display(fig)

